---
layout: post
title:  "Kaggle Otto Competition Finished"
date:   2015-05-19 12:30:08
categories: kaggle
---

My first Kaggle competition has finished. [Otto Group Product Classification Challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge/leaderboard/public).   791th position in private LB is somewhere in top 25% in something that seems like one of the busiest competition on the platform. I was running for top 10% at some point, but after first rush of addictive kaggling faded away I didn't check back before final week warning email and then was pushed back in LB quite a bit and didn't have enough time to really get into it.

Initial impressions:

- Fell back a bit compared to public LB, so must have had some overfitting
- Expanded my analytics toolkit with [xgboost](https://github.com/dmlc/xgboost/) and [lasagne](https://github.com/Lasagne/Lasagne)/[nolearn](https://github.com/dnouri/nolearn). Had been only working with sklearn before
- Did some aggressive parameter tuning. This competition mostly seemed to be about that (based on discussions in forums).
- And for the first time I got serious with combining together different models (ensemble) - still a fair bit to practice in that area
- Definitely want to see solutions of top teams- they should be published as I understand
- my ipython notebooks are at [github](https://github.com/saarlo/kaggle_otto)
- There are couple of interesting competitions already out that I will probably participate in. [Crowdflower](https://www.kaggle.com/c/crowdflower-search-relevance) competition seems to be interesting and involve some NLP 